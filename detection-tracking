#!/usr/bin/env python3
# -- coding: utf-8 --

import detectron2
import numpy as np
import imutils
import cv2
import time
from detectron2.utils.logger import setup_logger
from centroidtracker import CentroidTracker
import imutils
setup_logger()
from matplotlib import pyplot as plt
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog
from detectron2.structures import Boxes, BoxMode, pairwise_iou

cap = cv2.VideoCapture('Video-Prueba-1.mp4')
FPS = cap.get(cv2.CAP_PROP_FPS)
hasFrame, frame = cap.read()
ct = CentroidTracker()
frame_width = frame.shape[1]#int(cap.get(3))
frame_height = frame.shape[0]#int(cap.get(4))
video_writer = cv2.VideoWriter('Salida.avi', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), cap.get(cv2.CAP_PROP_FPS), (frame_width, frame_height))

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)

while cv2.waitKey(1) < 0:
    t = time.time()
    hasFrame, frame = cap.read()
    #frame = imutils.resize(frame, width=600)
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    if not hasFrame:
        cv2.waitKey()
        break

    outputs = predictor(frame)
    v = Visualizer(frame[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
    v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    img = v.get_image()[:, :, ::-1]
    imagen = np.copy(img)
    instances = outputs['instances']
    scores = instances.scores.cpu().numpy()

    #dim = ( frame_width, frame_height)
    #resized = cv2.resize(imagen,dim, interpolation = cv2.INTER_AREA)
    #if W is None or H is None:
        #(H, W) = frame.shape[:2]

    cuadra = []
    rects = []
    valor = 0.90
    print('Numero de Identificados: ', len(instances))
    i = 0
	  # loop over the detections
    for i in range(0, len(instances)):
        confidence = scores[i]
        if confidence > valor:
          #Boxes
          boxes = instances.pred_boxes.tensor.cpu().numpy()
          boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)
          boxes = boxes.tolist()
          print('Coordenadas Recuadros')
          print(str(i)+'-',boxes[i])
          #Keypoints
          has_keypoints = instances.has("pred_keypoints")
          if has_keypoints:
            keypoints = instances.pred_keypoints.cpu().numpy()
            keypoints = keypoints[:, :, :2] 
            keypoints = keypoints.tolist()
          print('Keypoints identificados')
          print(str(i)+'-',keypoints[i])
          #Tracking
          (x, y, w, h)=boxes[i]
          cuadra.append([x,y,x+w,y+h])
          box = cuadra[i] * np.array([1, 1, 1, 1])
          rects.append(box.astype("int"))
          (startX, startY, endX, endY) = box.astype("int")
          #print("hhhh",startX)
          cv2.rectangle(frame, (startX, startY), ((endX), (endY)),(0, 255, 0), 2)
	  
    # update our centroid tracker using the computed set of bounding
	  # box rectangles
    objects = ct.update(rects)

	  # loop over the tracked objects
    for (objectID, centroid) in objects.items():
        # draw both the ID of the object and the centroid of the
        # object on the output frame
        text = "Id{}".format(objectID)
        cv2.putText(frame, text, (centroid[0] - 20, centroid[1] - 20),
			cv2.FONT_HERSHEY_COMPLEX, 0.4, (0, 255, 0), 1, lineType=cv2.LINE_AA)
        cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)

	  # show the output frame
    cv2.imwrite("Frame.png", frame)
    video_writer.write(frame)
	  
cap.release()
video_writer.release()
cv2.destroyAllWindows()
